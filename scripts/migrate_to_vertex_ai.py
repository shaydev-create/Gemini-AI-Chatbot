#!/usr/bin/env python3
"""
Script de migraci√≥n autom√°tica a Google Cloud Vertex AI
Migra desde Gemini API gratuita a Vertex AI con fallback
Actualizado para usar el nuevo cliente Vertex AI
"""

import logging
import os
import sys
from pathlib import Path

# Configurar logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def get_project_root() -> Path:
    """Obtener la ruta ra√≠z del proyecto.

    Returns:
        Path: Ruta al directorio ra√≠z del proyecto
    """
    current_path = Path(__file__).parent

    # Buscar hacia arriba hasta encontrar requirements.txt o .git
    while current_path.parent != current_path:
        if (current_path / "requirements.txt").exists() or (
            current_path / ".git"
        ).exists():
            return current_path
        current_path = current_path.parent

    return Path(__file__).parent.parent


def check_requirements() -> bool:
    """Verificar requisitos previos.

    Returns:
        bool: True si todos los requisitos est√°n satisfechos
    """
    logger.info("üîç Verificando requisitos...")

    requirements = {
        "google-cloud-aiplatform": "google-cloud-aiplatform",
        "vertexai": "vertexai",
        "google-auth": "google-auth",
        "google-auth-oauthlib": "google-auth-oauthlib",
        "google-generativeai": "google-generativeai",
    }

    missing = []
    for package, pip_name in requirements.items():
        try:
            __import__(package.replace("-", "_"))
            logger.info(f"‚úÖ {package}")
        except ImportError:
            logger.error(f"‚ùå {package}")
            missing.append(pip_name)

    if missing:
        logger.error("\nüì¶ Dependencias faltantes. Instalar con:")
        logger.error(f"pip install {' '.join(missing)}")
        return False

    logger.info("‚úÖ Todos los requisitos est√°n satisfechos")
    return True


def update_vertex_ai_integration() -> bool:
    """Actualizar integraci√≥n con Vertex AI.

    Returns:
        bool: True si la actualizaci√≥n fue exitosa
    """
    logger.info("üîß Actualizando integraci√≥n con Vertex AI...")

    try:
        # Verificar que los archivos necesarios existen
        project_root = get_project_root()

        required_files = ["src/config/vertex_ai.py", "src/config/vertex_client.py"]

        for file_path in required_files:
            full_path = project_root / file_path
            if not full_path.exists():
                logger.error(f"‚ùå Archivo requerido no encontrado: {file_path}")
                return False
            logger.info(f"‚úÖ {file_path} encontrado")

        logger.info("‚úÖ Integraci√≥n con Vertex AI verificada")
        return True

    except Exception as e:
        logger.error(f"‚ùå Error actualizando integraci√≥n: {e}")
        return False


def create_vertex_ai_config():
    """Crear configuraci√≥n para Vertex AI (legacy - mantenido por compatibilidad)"""
    logger.warning("‚ö†Ô∏è Esta funci√≥n es legacy. Usar src/config/vertex_ai.py")

    config_content = '''"""
Configuraci√≥n para Google Cloud Vertex AI
"""

import os
from google.cloud import aiplatform
from google.auth import default

class VertexAIConfig:
    def __init__(self):
        self.project_id = os.getenv('GOOGLE_CLOUD_PROJECT_ID', 'tu-proyecto-id')
        self.location = os.getenv('GOOGLE_CLOUD_LOCATION', 'us-central1')
        self.credentials_path = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')

        # Modelos disponibles
        self.models = {
            'fast': 'gemini-1.5-flash',
            'pro': 'gemini-1.5-pro',
            'basic': 'gemini-1.0-pro'
        }

        # Configuraci√≥n de costos (USD por 1M tokens)
        self.costs = {
            'gemini-1.5-flash': 0.50,
            'gemini-1.5-pro': 3.50,
            'gemini-1.0-pro': 0.25
        }

        # L√≠mites de uso
        self.limits = {
            'requests_per_minute': 1000,
            'tokens_per_request': 32000,
            'max_daily_cost': 50.0  # USD
        }

    def initialize(self):
        """Inicializar Vertex AI"""
        try:
            if self.credentials_path and os.path.exists(self.credentials_path):
                os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = self.credentials_path

            aiplatform.init(
                project=self.project_id,
                location=self.location
            )
            return True
        except Exception as e:
            print(f"Error inicializando Vertex AI: {e}")
            return False

    def get_model_endpoint(self, model_type='fast'):
        """Obtener endpoint del modelo"""
        model_name = self.models.get(model_type, self.models['fast'])
        return f"projects/{self.project_id}/locations/{self.location}/publishers/google/models/{model_name}"

# Configuraci√≥n global
vertex_config = VertexAIConfig()
'''

    with open("src/config/vertex_ai.py", "w", encoding="utf-8") as f:
        f.write(config_content)

    print("‚úÖ Configuraci√≥n Vertex AI creada: src/config/vertex_ai.py")


def create_vertex_ai_client():
    """Crear cliente para Vertex AI (legacy - mantenido por compatibilidad)"""
    logger.warning("‚ö†Ô∏è Esta funci√≥n es legacy. Usar src/config/vertex_client.py")

    client_content = '''"""
Cliente para Google Cloud Vertex AI
Maneja requests con fallback autom√°tico
"""

import asyncio
import json
import time
from typing import Dict, List, Optional, Union
from google.cloud import aiplatform
from vertexai.generative_models import GenerativeModel
import vertexai

from .vertex_ai import vertex_config
from .gemini_client import GeminiClient  # Fallback

class VertexAIClient:
    def __init__(self):
        self.config = vertex_config
        self.fallback_client = GeminiClient()  # Cliente Gemini original
        self.initialized = False
        self.daily_cost = 0.0
        self.request_count = 0
        self.last_reset = time.time()

    async def initialize(self):
        """Inicializar cliente Vertex AI"""
        try:
            # Inicializar Vertex AI
            vertexai.init(
                project=self.config.project_id,
                location=self.config.location
            )

            # Crear modelos
            self.models = {
                'fast': GenerativeModel(self.config.models['fast']),
                'pro': GenerativeModel(self.config.models['pro']),
                'basic': GenerativeModel(self.config.models['basic'])
            }

            self.initialized = True
            print("‚úÖ Vertex AI inicializado correctamente")
            return True

        except Exception as e:
            print(f"‚ùå Error inicializando Vertex AI: {e}")
            print("üîÑ Usando Gemini API como fallback")
            return False

    def _reset_daily_limits(self):
        """Resetear l√≠mites diarios"""
        current_time = time.time()
        if current_time - self.last_reset > 86400:  # 24 horas
            self.daily_cost = 0.0
            self.request_count = 0
            self.last_reset = current_time

    def _calculate_cost(self, input_tokens: int, output_tokens: int, model: str) -> float:
        """Calcular costo del request"""
        total_tokens = input_tokens + output_tokens
        cost_per_million = self.config.costs.get(model, 0.50)
        return (total_tokens / 1_000_000) * cost_per_million

    def _check_limits(self, estimated_tokens: int) -> bool:
        """Verificar l√≠mites de uso"""
        self._reset_daily_limits()

        # Verificar l√≠mite de costo diario
        estimated_cost = (estimated_tokens / 1_000_000) * 0.50  # Estimaci√≥n conservadora
        if self.daily_cost + estimated_cost > self.config.limits['max_daily_cost']:
            return False

        return True

    async def generate_response(
        self,
        prompt: str,
        model_type: str = 'fast',
        max_tokens: int = 1000,
        temperature: float = 0.7,
        **kwargs
    ) -> Dict:
        """Generar respuesta con Vertex AI"""

        if not self.initialized:
            await self.initialize()

        # Si Vertex AI no est√° disponible, usar fallback
        if not self.initialized:
            return await self.fallback_client.generate_response(
                prompt, max_tokens=max_tokens, temperature=temperature, **kwargs
            )

        # Verificar l√≠mites
        estimated_tokens = len(prompt.split()) * 1.3 + max_tokens
        if not self._check_limits(estimated_tokens):
            print("‚ö†Ô∏è L√≠mite diario alcanzado, usando fallback")
            return await self.fallback_client.generate_response(
                prompt, max_tokens=max_tokens, temperature=temperature, **kwargs
            )

        try:
            # Seleccionar modelo
            model = self.models.get(model_type, self.models['fast'])

            # Configurar generaci√≥n
            generation_config = {
                'max_output_tokens': max_tokens,
                'temperature': temperature,
                'top_p': kwargs.get('top_p', 0.8),
                'top_k': kwargs.get('top_k', 40)
            }

            # Generar respuesta
            start_time = time.time()
            response = await model.generate_content_async(
                prompt,
                generation_config=generation_config
            )

            # Procesar respuesta
            response_time = time.time() - start_time
            response_text = response.text

            # Calcular m√©tricas
            input_tokens = len(prompt.split()) * 1.3  # Estimaci√≥n
            output_tokens = len(response_text.split()) * 1.3
            cost = self._calculate_cost(input_tokens, output_tokens, self.config.models[model_type])

            # Actualizar contadores
            self.daily_cost += cost
            self.request_count += 1

            return {
                'response': response_text,
                'model': self.config.models[model_type],
                'tokens_used': int(input_tokens + output_tokens),
                'cost': cost,
                'response_time': response_time,
                'source': 'vertex_ai'
            }

        except Exception as e:
            print(f"‚ùå Error en Vertex AI: {e}")
            print("üîÑ Usando fallback...")
            return await self.fallback_client.generate_response(
                prompt, max_tokens=max_tokens, temperature=temperature, **kwargs
            )

    async def generate_stream_response(self, prompt: str, **kwargs):
        """Generar respuesta en streaming"""
        # Implementar streaming si es necesario
        response = await self.generate_response(prompt, **kwargs)
        yield response['response']

    def get_usage_stats(self) -> Dict:
        """Obtener estad√≠sticas de uso"""
        self._reset_daily_limits()

        return {
            'daily_requests': self.request_count,
            'daily_cost': round(self.daily_cost, 4),
            'cost_limit': self.config.limits['max_daily_cost'],
            'cost_remaining': round(self.config.limits['max_daily_cost'] - self.daily_cost, 4),
            'percentage_used': round((self.daily_cost / self.config.limits['max_daily_cost']) * 100, 2)
        }

# Cliente global
vertex_client = VertexAIClient()
'''

    os.makedirs("src/config", exist_ok=True)
    with open("src/config/vertex_client.py", "w", encoding="utf-8") as f:
        f.write(client_content)

    print("‚úÖ Cliente Vertex AI creado: src/config/vertex_client.py")


def create_env_template():
    """Crear template de variables de entorno"""

    env_content = """# Google Cloud Vertex AI Configuration
GOOGLE_CLOUD_PROJECT_ID=tu-proyecto-id
GOOGLE_CLOUD_LOCATION=us-central1
GOOGLE_APPLICATION_CREDENTIALS=path/to/service-account-key.json

# Vertex AI Settings
VERTEX_AI_ENABLED=true
VERTEX_AI_DEFAULT_MODEL=fast
VERTEX_AI_MAX_DAILY_COST=50.0

# Fallback Configuration
ENABLE_FALLBACK=true
FALLBACK_ORDER=vertex_ai,gemini_api,openai

# Monitoring
ENABLE_USAGE_TRACKING=true
COST_ALERT_THRESHOLD=80
"""

    with open(".env.vertex", "w", encoding="utf-8") as f:
        f.write(env_content)

    print("‚úÖ Template de variables creado: .env.vertex")


def create_migration_guide():
    """Crear gu√≠a de migraci√≥n"""

    guide_content = """# üöÄ Gu√≠a de Migraci√≥n a Vertex AI - Pasos Espec√≠ficos

## üìã CHECKLIST DE MIGRACI√ìN

### ‚úÖ Paso 1: Configuraci√≥n Google Cloud (15 minutos)

1. **Crear proyecto en Google Cloud**
   - Ir a: https://console.cloud.google.com
   - Crear nuevo proyecto: "gemini-ai-chatbot-prod"
   - Anotar el PROJECT_ID

2. **Habilitar APIs necesarias**
   ```bash
   gcloud services enable aiplatform.googleapis.com
   gcloud services enable compute.googleapis.com
   ```

3. **Configurar billing**
   - Vincular tarjeta de cr√©dito
   - Configurar alertas de billing ($10, $25, $50)

### ‚úÖ Paso 2: Crear Service Account (10 minutos)

1. **Crear service account**
   - IAM & Admin > Service Accounts
   - Crear nueva cuenta: "vertex-ai-service"
   - Roles: "Vertex AI User", "AI Platform Developer"

2. **Descargar JSON key**
   - Crear nueva key (JSON)
   - Guardar como: `credentials/vertex-ai-key.json`
   - ‚ö†Ô∏è NUNCA subir a Git

### ‚úÖ Paso 3: Configurar Variables de Entorno (5 minutos)

1. **Copiar template**
   ```bash
   cp .env.vertex .env.production
   ```

2. **Editar variables**
   ```bash
   GOOGLE_CLOUD_PROJECT_ID=tu-proyecto-real-id
   GOOGLE_APPLICATION_CREDENTIALS=./credentials/vertex-ai-key.json
   ```

### ‚úÖ Paso 4: Instalar Dependencias (5 minutos)

```bash
pip install google-cloud-aiplatform
pip install vertexai
pip install google-auth
```

### ‚úÖ Paso 5: Testing (30 minutos)

1. **Test b√°sico**
   ```python
   from app.config.vertex_client import vertex_client

   # Test de conexi√≥n
   await vertex_client.initialize()

   # Test de respuesta
   response = await vertex_client.generate_response("Hola, ¬øc√≥mo est√°s?")
   print(response)
   ```

2. **Test de fallback**
   - Desconectar internet
   - Verificar que usa Gemini API

### ‚úÖ Paso 6: Deploy Gradual (1 d√≠a)

1. **Deploy en staging**
   - Configurar variables de entorno
   - Test completo de funcionalidad

2. **Deploy en producci√≥n**
   - A/B testing: 10% Vertex AI
   - Monitorear errores y costos
   - Incrementar gradualmente

## üîß COMANDOS √öTILES

### Verificar configuraci√≥n
```bash
gcloud auth list
gcloud config list project
gcloud services list --enabled
```

### Monitorear costos
```bash
gcloud billing budgets list
gcloud logging read "resource.type=vertex_ai"
```

### Troubleshooting
```bash
# Verificar permisos
gcloud auth application-default login

# Test de API
gcloud ai models list --region=us-central1
```

## üìä MONITOREO POST-MIGRACI√ìN

### M√©tricas a vigilar (primeros 7 d√≠as)
- ‚úÖ Tasa de √©xito de requests
- ‚úÖ Latencia promedio
- ‚úÖ Costo diario
- ‚úÖ Uso de fallback

### Alertas recomendadas
- üö® Costo diario > $10
- üö® Tasa de error > 5%
- üö® Latencia > 5 segundos

## üéØ RESULTADOS ESPERADOS

### Mejoras inmediatas
- ‚ö° Latencia: 2-3 segundos (vs 5-10 actual)
- üîÑ Confiabilidad: 99.9% uptime
- üìà L√≠mites: 1000 req/min (vs 15 actual)
- üí∞ Costo: $15-50/mes (escalable)

### Beneficios a largo plazo
- üöÄ Escalabilidad autom√°tica
- üìä Analytics detallados
- üõ°Ô∏è SLA garantizado
- üîß Soporte t√©cnico

## ‚ùì PREGUNTAS FRECUENTES

**Q: ¬øQu√© pasa si se agota el presupuesto?**
A: Autom√°ticamente cambia a Gemini API gratuita.

**Q: ¬øPuedo cambiar de modelo din√°micamente?**
A: S√≠, puedes usar 'fast', 'pro' o 'basic' seg√∫n necesidad.

**Q: ¬øC√≥mo monitoreo los costos?**
A: Dashboard en Google Cloud + alertas autom√°ticas.

**Q: ¬øEs reversible la migraci√≥n?**
A: S√≠, puedes volver a Gemini API en cualquier momento.
"""

    with open("docs/VERTEX_AI_MIGRATION_STEPS.md", "w", encoding="utf-8") as f:
        f.write(guide_content)

    print("‚úÖ Gu√≠a de migraci√≥n creada: docs/VERTEX_AI_MIGRATION_STEPS.md")


def test_vertex_ai_integration() -> bool:
    """Probar la integraci√≥n con Vertex AI.

    Returns:
        bool: True si la integraci√≥n funciona correctamente
    """
    logger.info("üß™ Probando integraci√≥n con Vertex AI...")

    try:
        # Importar el nuevo cliente
        sys.path.insert(0, str(get_project_root()))
        # Intentar inicializar
        import asyncio

        from app.config.vertex_client import vertex_client

        async def test_client():
            success = await vertex_client.initialize()
            if success:
                logger.info("‚úÖ Cliente Vertex AI inicializado")

                # Obtener estad√≠sticas
                stats = vertex_client.get_usage_stats()
                logger.info(f"üìä Estado: {stats['status']}")

                return True
            else:
                logger.warning("‚ö†Ô∏è Vertex AI no disponible, fallback activo")
                return False

        result = asyncio.run(test_client())
        return result

    except Exception as e:
        logger.error(f"‚ùå Error probando integraci√≥n: {e}")
        return False


def main():
    """Funci√≥n principal del script de migraci√≥n"""
    import argparse

    parser = argparse.ArgumentParser(description="Migraci√≥n a Google Cloud Vertex AI")
    parser.add_argument(
        "--check", action="store_true", help="Solo verificar requisitos y configuraci√≥n"
    )
    parser.add_argument(
        "--test", action="store_true", help="Probar integraci√≥n con Vertex AI"
    )
    parser.add_argument(
        "--setup",
        action="store_true",
        help="Configurar archivos legacy (no recomendado)",
    )

    args = parser.parse_args()

    logger.info("üöÄ Migraci√≥n a Google Cloud Vertex AI")
    logger.info("=" * 40)

    try:
        # 1. Verificar requisitos
        if not check_requirements():
            logger.error("‚ùå Requisitos no satisfechos")
            sys.exit(1)

        # 2. Verificar integraci√≥n actual
        if not update_vertex_ai_integration():
            logger.error("‚ùå Integraci√≥n no disponible")
            sys.exit(1)

        # Ejecutar acciones seg√∫n argumentos
        if args.check:
            logger.info("‚úÖ Verificaci√≥n completada")
            return

        if args.test:
            if test_vertex_ai_integration():
                logger.info("‚úÖ Integraci√≥n funcionando correctamente")
            else:
                logger.warning("‚ö†Ô∏è Integraci√≥n con problemas")
            return

        if args.setup:
            logger.warning("‚ö†Ô∏è Configurando archivos legacy...")
            create_vertex_ai_config()
            create_vertex_ai_client()
            create_env_template()

        # Mostrar gu√≠a por defecto
        create_migration_guide()

        logger.info("\n‚úÖ Proceso completado exitosamente!")
        logger.info("\nüìã Pr√≥ximos pasos:")
        logger.info("1. Configurar variables de entorno (.env)")
        logger.info("2. Configurar Google Cloud credentials")
        logger.info("3. Ejecutar: python scripts/migrate_to_vertex_ai.py --test")
        logger.info("4. Usar vertex_client en tu aplicaci√≥n")

    except Exception as e:
        logger.error(f"\n‚ùå Error durante la migraci√≥n: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
